{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1u7W7qT3uBgR/KSdCrLFl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achiyechezkel/machine-learning-NN/blob/main/achinoam_yechezkel_ex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5svDfxyVYW7J"
      },
      "outputs": [],
      "source": [
        "https://github.com/achiyechezkel/machine-learning-NN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Replace this ID with your own ID number\n",
        "your_id = \"212305510\"\n",
        "\n",
        "# Extract the number of neurons for each layer from the ID\n",
        "neurons_fc1 = int(your_id[-2:])       # Last pair of digits\n",
        "neurons_fc2 = int(your_id[-4:-2])    # Second last pair of digits\n",
        "neurons_fc3 = int(your_id[-6:-4])    # Third last pair of digits\n",
        "neurons_fc4 = int(your_id[-8:-6])    # Fourth last pair of digits\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten the images for the Dense layers\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the network\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28 * 28,)),\n",
        "    Dense(neurons_fc1, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(neurons_fc2, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(neurons_fc3, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(neurons_fc4, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcw1KWDPZVLb",
        "outputId": "81b5c0d6-83f3-4ea9-c003-d129e5f4aa08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.2608 - loss: 2.1464 - val_accuracy: 0.7920 - val_loss: 0.8547\n",
            "Epoch 2/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5849 - loss: 1.2504 - val_accuracy: 0.8640 - val_loss: 0.5480\n",
            "Epoch 3/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6668 - loss: 1.0160 - val_accuracy: 0.8831 - val_loss: 0.4510\n",
            "Epoch 4/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7053 - loss: 0.9232 - val_accuracy: 0.8941 - val_loss: 0.3981\n",
            "Epoch 5/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7259 - loss: 0.8592 - val_accuracy: 0.8999 - val_loss: 0.3676\n",
            "Epoch 6/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7395 - loss: 0.8210 - val_accuracy: 0.9024 - val_loss: 0.3565\n",
            "Epoch 7/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7462 - loss: 0.8039 - val_accuracy: 0.9053 - val_loss: 0.3445\n",
            "Epoch 8/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7578 - loss: 0.7743 - val_accuracy: 0.9064 - val_loss: 0.3354\n",
            "Epoch 9/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7633 - loss: 0.7680 - val_accuracy: 0.9073 - val_loss: 0.3301\n",
            "Epoch 10/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7728 - loss: 0.7386 - val_accuracy: 0.9065 - val_loss: 0.3290\n",
            "Epoch 11/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7756 - loss: 0.7232 - val_accuracy: 0.9068 - val_loss: 0.3220\n",
            "Epoch 12/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7769 - loss: 0.7265 - val_accuracy: 0.9067 - val_loss: 0.3254\n",
            "Epoch 13/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7786 - loss: 0.7226 - val_accuracy: 0.9105 - val_loss: 0.3108\n",
            "Epoch 14/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7804 - loss: 0.7188 - val_accuracy: 0.9092 - val_loss: 0.3198\n",
            "Epoch 15/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7815 - loss: 0.7110 - val_accuracy: 0.9107 - val_loss: 0.3199\n",
            "Epoch 16/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7819 - loss: 0.7096 - val_accuracy: 0.9080 - val_loss: 0.3173\n",
            "Epoch 17/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7867 - loss: 0.6948 - val_accuracy: 0.9103 - val_loss: 0.3141\n",
            "Epoch 18/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7810 - loss: 0.7095 - val_accuracy: 0.9087 - val_loss: 0.3189\n",
            "Epoch 19/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7879 - loss: 0.7003 - val_accuracy: 0.9118 - val_loss: 0.3068\n",
            "Epoch 20/20\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7886 - loss: 0.6865 - val_accuracy: 0.9125 - val_loss: 0.3064\n",
            "Test Accuracy: 0.9121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "\n",
        "your_id = \"212305510\"\n",
        "\n",
        "neurons_fc1 = int(your_id[-2:])\n",
        "neurons_fc2 = int(your_id[-4:-2])\n",
        "neurons_fc3 = int(your_id[-6:-4])\n",
        "neurons_fc4 = int(your_id[-8:-6])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "def build_model(dropout_rate, learning_rate, batch_size):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28 * 28,)),\n",
        "        Dense(neurons_fc1, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(neurons_fc2, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(neurons_fc3, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(neurons_fc4, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model, batch_size\n",
        "\n",
        "param_grid = [\n",
        "    {'dropout_rate': dr, 'learning_rate': lr, 'batch_size': bs}\n",
        "    for dr in [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    for lr in [0.001, 0.0005, 0.0001]\n",
        "    for bs in [64, 128, 256]\n",
        "]\n",
        "\n",
        "print(f\"Total experiments to run: {len(param_grid)}\")\n",
        "\n",
        "best_result = None\n",
        "best_accuracy = 0\n",
        "results = []\n",
        "for i, params in enumerate(param_grid):\n",
        "    print(f\"Running experiment {i+1} with params: {params}\")\n",
        "    model, batch_size = build_model(params['dropout_rate'], params['learning_rate'], params['batch_size'])\n",
        "    history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    results.append({\n",
        "        'Experiment': i+1,\n",
        "        'Dropout Rate': params['dropout_rate'],\n",
        "        'Learning Rate': params['learning_rate'],\n",
        "        'Batch Size': params['batch_size'],\n",
        "        'Test Accuracy': accuracy\n",
        "    })\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_result = {**params, 'accuracy': accuracy}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(results_df)\n",
        "\n",
        "print(\"\\nBest Experiment:\")\n",
        "print(best_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2u0sbYxkR0z",
        "outputId": "e8b2a966-c0c8-4040-9de9-d584df2cf190"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total experiments to run: 45\n",
            "Running experiment 1 with params: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 64}\n",
            "Running experiment 2 with params: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 128}\n",
            "Running experiment 3 with params: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 256}\n",
            "Running experiment 4 with params: {'dropout_rate': 0.1, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Running experiment 5 with params: {'dropout_rate': 0.1, 'learning_rate': 0.0005, 'batch_size': 128}\n",
            "Running experiment 6 with params: {'dropout_rate': 0.1, 'learning_rate': 0.0005, 'batch_size': 256}\n",
            "Running experiment 7 with params: {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 64}\n",
            "Running experiment 8 with params: {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Running experiment 9 with params: {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 256}\n",
            "Running experiment 10 with params: {'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 64}\n",
            "Running experiment 11 with params: {'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 128}\n",
            "Running experiment 12 with params: {'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 256}\n",
            "Running experiment 13 with params: {'dropout_rate': 0.2, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Running experiment 14 with params: {'dropout_rate': 0.2, 'learning_rate': 0.0005, 'batch_size': 128}\n",
            "Running experiment 15 with params: {'dropout_rate': 0.2, 'learning_rate': 0.0005, 'batch_size': 256}\n",
            "Running experiment 16 with params: {'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 64}\n",
            "Running experiment 17 with params: {'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Running experiment 18 with params: {'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 256}\n",
            "Running experiment 19 with params: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 64}\n",
            "Running experiment 20 with params: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 128}\n",
            "Running experiment 21 with params: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 256}\n",
            "Running experiment 22 with params: {'dropout_rate': 0.3, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Running experiment 23 with params: {'dropout_rate': 0.3, 'learning_rate': 0.0005, 'batch_size': 128}\n",
            "Running experiment 24 with params: {'dropout_rate': 0.3, 'learning_rate': 0.0005, 'batch_size': 256}\n",
            "Running experiment 25 with params: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'batch_size': 64}\n",
            "Running experiment 26 with params: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Running experiment 27 with params: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'batch_size': 256}\n",
            "Running experiment 28 with params: {'dropout_rate': 0.4, 'learning_rate': 0.001, 'batch_size': 64}\n",
            "Running experiment 29 with params: {'dropout_rate': 0.4, 'learning_rate': 0.001, 'batch_size': 128}\n",
            "Running experiment 30 with params: {'dropout_rate': 0.4, 'learning_rate': 0.001, 'batch_size': 256}\n",
            "Running experiment 31 with params: {'dropout_rate': 0.4, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Running experiment 32 with params: {'dropout_rate': 0.4, 'learning_rate': 0.0005, 'batch_size': 128}\n",
            "Running experiment 33 with params: {'dropout_rate': 0.4, 'learning_rate': 0.0005, 'batch_size': 256}\n",
            "Running experiment 34 with params: {'dropout_rate': 0.4, 'learning_rate': 0.0001, 'batch_size': 64}\n",
            "Running experiment 35 with params: {'dropout_rate': 0.4, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Running experiment 36 with params: {'dropout_rate': 0.4, 'learning_rate': 0.0001, 'batch_size': 256}\n",
            "Running experiment 37 with params: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'batch_size': 64}\n",
            "Running experiment 38 with params: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'batch_size': 128}\n",
            "Running experiment 39 with params: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'batch_size': 256}\n",
            "Running experiment 40 with params: {'dropout_rate': 0.5, 'learning_rate': 0.0005, 'batch_size': 64}\n",
            "Running experiment 41 with params: {'dropout_rate': 0.5, 'learning_rate': 0.0005, 'batch_size': 128}\n",
            "Running experiment 42 with params: {'dropout_rate': 0.5, 'learning_rate': 0.0005, 'batch_size': 256}\n",
            "Running experiment 43 with params: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'batch_size': 64}\n",
            "Running experiment 44 with params: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'batch_size': 128}\n",
            "Running experiment 45 with params: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'batch_size': 256}\n",
            "    Experiment  Dropout Rate  Learning Rate  Batch Size  Test Accuracy\n",
            "0            1           0.1         0.0010          64         0.9382\n",
            "1            2           0.1         0.0010         128         0.9299\n",
            "2            3           0.1         0.0010         256         0.9274\n",
            "3            4           0.1         0.0005          64         0.9316\n",
            "4            5           0.1         0.0005         128         0.9311\n",
            "5            6           0.1         0.0005         256         0.9300\n",
            "6            7           0.1         0.0001          64         0.9053\n",
            "7            8           0.1         0.0001         128         0.8952\n",
            "8            9           0.1         0.0001         256         0.8692\n",
            "9           10           0.2         0.0010          64         0.9123\n",
            "10          11           0.2         0.0010         128         0.9200\n",
            "11          12           0.2         0.0010         256         0.9219\n",
            "12          13           0.2         0.0005          64         0.9166\n",
            "13          14           0.2         0.0005         128         0.9065\n",
            "14          15           0.2         0.0005         256         0.9134\n",
            "15          16           0.2         0.0001          64         0.8847\n",
            "16          17           0.2         0.0001         128         0.8372\n",
            "17          18           0.2         0.0001         256         0.7960\n",
            "18          19           0.3         0.0010          64         0.9068\n",
            "19          20           0.3         0.0010         128         0.8556\n",
            "20          21           0.3         0.0010         256         0.8828\n",
            "21          22           0.3         0.0005          64         0.8969\n",
            "22          23           0.3         0.0005         128         0.8926\n",
            "23          24           0.3         0.0005         256         0.8850\n",
            "24          25           0.3         0.0001          64         0.8728\n",
            "25          26           0.3         0.0001         128         0.7435\n",
            "26          27           0.3         0.0001         256         0.7861\n",
            "27          28           0.4         0.0010          64         0.7904\n",
            "28          29           0.4         0.0010         128         0.8367\n",
            "29          30           0.4         0.0010         256         0.7859\n",
            "30          31           0.4         0.0005          64         0.7971\n",
            "31          32           0.4         0.0005         128         0.7337\n",
            "32          33           0.4         0.0005         256         0.6933\n",
            "33          34           0.4         0.0001          64         0.7386\n",
            "34          35           0.4         0.0001         128         0.7197\n",
            "35          36           0.4         0.0001         256         0.6073\n",
            "36          37           0.5         0.0010          64         0.6918\n",
            "37          38           0.5         0.0010         128         0.7030\n",
            "38          39           0.5         0.0010         256         0.5944\n",
            "39          40           0.5         0.0005          64         0.6453\n",
            "40          41           0.5         0.0005         128         0.5048\n",
            "41          42           0.5         0.0005         256         0.5849\n",
            "42          43           0.5         0.0001          64         0.7069\n",
            "43          44           0.5         0.0001         128         0.6634\n",
            "44          45           0.5         0.0001         256         0.5135\n",
            "\n",
            "Best Experiment:\n",
            "{'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 64, 'accuracy': 0.9381999969482422}\n"
          ]
        }
      ]
    }
  ]
}